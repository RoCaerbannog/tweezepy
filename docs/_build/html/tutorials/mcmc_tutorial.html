
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Monte carlo sampler for uncertainty &#8212; python 0.1.2 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Quickstart" href="quickstart.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <span class="target" id="module-tweezepy"></span><p><strong>Note:</strong> This tutorial was generated from an IPython notebook that can be
downloaded <a class="reference external" href="../../_static/notebooks/mcmc_tutorial.ipynb">here</a>.</p>
<div class="section" id="monte-carlo-sampler-for-uncertainty">
<span id="mcmc-tutorial"></span><h1>Monte carlo sampler for uncertainty<a class="headerlink" href="#monte-carlo-sampler-for-uncertainty" title="Permalink to this headline">¶</a></h1>
<p>For most uses, the Hessian method should be sufficient for calculating
parameter uncertainties. However, it is possible that instrumental noise
could cause non-linear effects that disrupt the gaussian approximation
in the Hessian method. For these cases, tweezepy comes built in with a
Markov chain Monte Carlo (MCMC) sampler that can sample the parameter
space to estimate non-gaussian noise.</p>
<p>Here, we demonstrate how to use the MCMC feature.</p>
<p>To start, we’ll simulate some data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tweezepy.smmcalibration</span> <span class="kn">import</span> <span class="n">PSD</span>
<span class="kn">from</span> <span class="nn">tweezepy.simulations</span> <span class="kn">import</span> <span class="n">downsampled_trace</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fc</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># corner frequency</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-5</span> <span class="c1"># dissipation due to viscous drag, in pN s/nm</span>
             <span class="c1"># 1e-5 is a typical value for an MT experiment</span>
<span class="n">kappa</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">fc</span> <span class="c1"># kappas in pN/nm</span>
<span class="n">fsample</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># sampling frequency in Hz</span>
<span class="n">N</span>  <span class="o">=</span> <span class="mi">10240</span> <span class="c1"># number of points in trace</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># gives the same random numbers each time</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">fsample</span> <span class="c1"># generate time for plotting purposes</span>
<span class="n">xtrace</span> <span class="o">=</span> <span class="n">downsampled_trace</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">kappa</span><span class="p">,</span><span class="n">fsample</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span> <span class="c1"># simulated the position data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">xtrace</span><span class="p">)</span> <span class="c1"># plot the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (s)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position (nm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_4_0.svg" src="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_4_0.svg" /><p>Now, we’ll calculate the power spectral densities and fit them using the
maximum likelihood estimation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">psd</span> <span class="o">=</span> <span class="n">PSD</span><span class="p">(</span><span class="n">xtrace</span><span class="p">,</span><span class="n">fsample</span><span class="p">)</span>
<span class="n">psd</span><span class="o">.</span><span class="n">mlefit</span><span class="p">()</span>
<span class="n">psd</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
<img alt="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_6_0.svg" src="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_6_0.svg" /><p>Next, we’ll run the MCMC sampler. The default values in tweezepy are 32
walkers and 1500 steps of the MCMC. On my computer, this usually takes
15 seconds. This is usually sufficient to get good estimates, but the
user can test out different values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">psd</span><span class="o">.</span><span class="n">mcmc</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="mi">1500</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s take a look at what the sampler has done. This usually happens
“under the hood,” but it’s useful to get a sense of how these things
work. The figure below shows the position of each walker as a function
of the number of steps in the chain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sampler</span> <span class="o">=</span> <span class="n">psd</span><span class="o">.</span><span class="n">sampler</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\kappa$&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">],</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;step number&quot;</span><span class="p">);</span>
</pre></div>
</div>
<img alt="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_10_0.svg" src="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_10_0.svg" /><p>The walkers start in small distributions around the maximum likelihood
values and then quickly wander and start exploring the full
distribution. After fewer than 50 steps, the samples are pretty well
“burnt-in.” To check this statement quantitatively, we can look at the
integrated autocorrelation time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tau</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_autocorr_time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">25.07859788</span> <span class="mf">22.33785708</span><span class="p">]</span>
</pre></div>
</div>
<p>This suggests that only about 25 steps are required for the chain to
“forget” where it started. So, we’ll throw away 4x this amount as
“burn-in” and thin the samples by about half the autocorrelation time
(10 steps).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">flat_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">(</span><span class="n">discard</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">flat</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">flat_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">4480</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h1>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h1>
<p>With out list of samples , we’ll take a look at a corner plot. To do so,
you’ll need the corner.py module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">corner</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">corner</span><span class="o">.</span><span class="n">corner</span><span class="p">(</span>
    <span class="n">flat_samples</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">truths</span><span class="o">=</span><span class="p">[</span><span class="n">alpha</span><span class="p">,</span> <span class="n">kappa</span><span class="p">],</span>
<span class="p">);</span>
</pre></div>
</div>
<img alt="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_16_0.svg" src="tutorials\mcmc_tutorial_files%5Cmcmc_tutorial_16_0.svg" /><p>This plot shows us all the one and two dimensional projections of the
probability distributions of the parameters. This allows us to see if
their are any covariances between parameters. From this, we see that the
distributions are fairly symmetrical, suggesting that the gaussian
approximation would be appropriate. It also shows us that the noise in
the simulation throws off our estimation of alpha slightly, but does a
good job of capturing kappa.</p>
<p>Typically, parameter errors are given as 1<span class="math notranslate nohighlight">\(\sigma\)</span>, so, in
analogy, we calculate the 16th, 50th, and 84th percentiles.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Math</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;\alpha&quot;</span><span class="p">,</span><span class="sa">r</span><span class="s2">&quot;\kappa&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">mcmc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">flat_samples</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">84</span><span class="p">])</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">mcmc</span><span class="p">)</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="s2">&quot;{{</span><span class="si">{3}</span><span class="s2">}} = </span><span class="si">{0:.3g}</span><span class="s2">_{{-</span><span class="si">{1:.3g}</span><span class="s2">}}^{{</span><span class="si">{2:.3g}</span><span class="s2">}}&quot;</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mcmc</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Math</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span>
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[\displaystyle {\alpha} = 9.8e-06_{-1.08e-07}^{1.07e-07}\]</div>
<div class="math notranslate nohighlight">
\[\displaystyle {\kappa} = 0.000618_{-1.7e-05}^{1.6e-05}\]</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">python</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Monte carlo sampler for uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="#results">Results</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="quickstart.html" title="previous chapter">Quickstart</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Ian L. Morgan.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/tutorials/mcmc_tutorial.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>